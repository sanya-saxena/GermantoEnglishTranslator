{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unicodedata import normalize\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n",
      "[go on] => [mach weiter]\n",
      "[hello] => [hallo]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "filename = 'deu.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading cleaned data\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda_folder\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2315\n",
      "English Max Length: 5\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 3686\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x1ea8cdb3ef0>,\n",
       "  <matplotlib.patches.Wedge at 0x1ea8cdbb400>],\n",
       " [Text(0.386335,1.02992,'English Words'),\n",
       "  Text(-0.386335,-1.02992,'German Words')])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Analysis of English and German Words\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title('Number of words present in the dataset')\n",
    "x = [eng_vocab_size,ger_vocab_size]\n",
    "plt.pie(x,labels = ['English Words','German Words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 10, 256)           943616    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 5, 2315)           594955    \n",
      "=================================================================\n",
      "Total params: 2,589,195\n",
      "Trainable params: 2,589,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('German: %s,\\nCorrect English Translation=%s,\\nPredicted English Translation=%s\\n' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU Score: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0),smoothing_function=SmoothingFunction().method4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATASET:\n",
      "\n",
      "German: ist das fur mich,\n",
      "Correct English Translation=is it for me,\n",
      "Predicted English Translation=punched punched punched punched punched\n",
      "\n",
      "German: ich habe kaviar gegessen,\n",
      "Correct English Translation=i ate caviar,\n",
      "Predicted English Translation=for for for for for\n",
      "\n",
      "German: das ist eine pflanze,\n",
      "Correct English Translation=its a plant,\n",
      "Predicted English Translation=foot brazil brazil brazil brazil\n",
      "\n",
      "German: das ist ein tisch,\n",
      "Correct English Translation=that is a table,\n",
      "Predicted English Translation=having having having having having\n",
      "\n",
      "German: sie schoss auf ihn,\n",
      "Correct English Translation=she shot him,\n",
      "Predicted English Translation=pick pick right right right\n",
      "\n",
      "German: du bist durchtrieben,\n",
      "Correct English Translation=youre sharp,\n",
      "Predicted English Translation=warm slapped slapped slapped slapped\n",
      "\n",
      "German: entledigen sie sich ihrer,\n",
      "Correct English Translation=get rid of her,\n",
      "Predicted English Translation=rice rice rice rice rice\n",
      "\n",
      "German: rate wer ich bin,\n",
      "Correct English Translation=guess who i am,\n",
      "Predicted English Translation=nothing nothing nothing remove remove\n",
      "\n",
      "German: tom kann antworten,\n",
      "Correct English Translation=tom can answer,\n",
      "Predicted English Translation=having having having having having\n",
      "\n",
      "German: ich bin so glucklich,\n",
      "Correct English Translation=im so happy,\n",
      "Predicted English Translation=aah aah aah aah aah\n",
      "\n",
      "BLEU Score: 0.022423\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences before training\n",
    "print('TRAINING DATASET:\\n')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING DATASET:\n",
      "\n",
      "German: tom kennt pistolen,\n",
      "Correct English Translation=tom knows guns,\n",
      "Predicted English Translation=behave behave thief thief thief\n",
      "\n",
      "German: tom ist brav,\n",
      "Correct English Translation=tom is good,\n",
      "Predicted English Translation=whose whose their their their\n",
      "\n",
      "German: tom hat verloren,\n",
      "Correct English Translation=tom lost,\n",
      "Predicted English Translation=undressing undressing undressing undressing undressing\n",
      "\n",
      "German: wer ist gefallen,\n",
      "Correct English Translation=who fell,\n",
      "Predicted English Translation=tell tell tell klutz klutz\n",
      "\n",
      "German: tom kennt mich,\n",
      "Correct English Translation=tom knows me,\n",
      "Predicted English Translation=inside cost seven seven seven\n",
      "\n",
      "German: sie sind wach,\n",
      "Correct English Translation=theyre awake,\n",
      "Predicted English Translation=obese obese obese obese obese\n",
      "\n",
      "German: dort ist tom,\n",
      "Correct English Translation=theres tom,\n",
      "Predicted English Translation=pie after after after after\n",
      "\n",
      "German: wir schauten nach oben,\n",
      "Correct English Translation=we looked up,\n",
      "Predicted English Translation=gasped gasped gasped gasped gasped\n",
      "\n",
      "German: kannst du mir helfen,\n",
      "Correct English Translation=can you help me,\n",
      "Predicted English Translation=girls girls girls girls girls\n",
      "\n",
      "German: es muss hier sein,\n",
      "Correct English Translation=it must be here,\n",
      "Predicted English Translation=my my my my my\n",
      "\n",
      "BLEU Score: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# test on some test sequences before training\n",
    "print('TESTING DATASET:\\n')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      " - 80s - loss: 3.9660 - val_loss: 3.4799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.47994, saving model to model.h5\n",
      "Epoch 2/20\n",
      " - 69s - loss: 3.2947 - val_loss: 3.2883\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.47994 to 3.28833, saving model to model.h5\n",
      "Epoch 3/20\n",
      " - 70s - loss: 3.0464 - val_loss: 3.1356\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.28833 to 3.13564, saving model to model.h5\n",
      "Epoch 4/20\n",
      " - 71s - loss: 2.8106 - val_loss: 2.9747\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.13564 to 2.97471, saving model to model.h5\n",
      "Epoch 5/20\n",
      " - 71s - loss: 2.6042 - val_loss: 2.8439\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.97471 to 2.84391, saving model to model.h5\n",
      "Epoch 6/20\n",
      " - 69s - loss: 2.3833 - val_loss: 2.6791\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.84391 to 2.67905, saving model to model.h5\n",
      "Epoch 7/20\n",
      " - 70s - loss: 2.1647 - val_loss: 2.5472\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.67905 to 2.54723, saving model to model.h5\n",
      "Epoch 8/20\n",
      " - 70s - loss: 1.9734 - val_loss: 2.4509\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.54723 to 2.45092, saving model to model.h5\n",
      "Epoch 9/20\n",
      " - 69s - loss: 1.7979 - val_loss: 2.3773\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.45092 to 2.37728, saving model to model.h5\n",
      "Epoch 10/20\n",
      " - 70s - loss: 1.6359 - val_loss: 2.3322\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.37728 to 2.33218, saving model to model.h5\n",
      "Epoch 11/20\n",
      " - 73s - loss: 1.4865 - val_loss: 2.2641\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.33218 to 2.26409, saving model to model.h5\n",
      "Epoch 12/20\n",
      " - 70s - loss: 1.3486 - val_loss: 2.2128\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.26409 to 2.21284, saving model to model.h5\n",
      "Epoch 13/20\n",
      " - 70s - loss: 1.2166 - val_loss: 2.1760\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.21284 to 2.17600, saving model to model.h5\n",
      "Epoch 14/20\n",
      " - 72s - loss: 1.0917 - val_loss: 2.1562\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.17600 to 2.15623, saving model to model.h5\n",
      "Epoch 15/20\n",
      " - 72s - loss: 0.9803 - val_loss: 2.1309\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.15623 to 2.13090, saving model to model.h5\n",
      "Epoch 16/20\n",
      " - 72s - loss: 0.8758 - val_loss: 2.1111\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.13090 to 2.11114, saving model to model.h5\n",
      "Epoch 17/20\n",
      " - 72s - loss: 0.7796 - val_loss: 2.0894\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.11114 to 2.08944, saving model to model.h5\n",
      "Epoch 18/20\n",
      " - 70s - loss: 0.6934 - val_loss: 2.0867\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.08944 to 2.08665, saving model to model.h5\n",
      "Epoch 19/20\n",
      " - 72s - loss: 0.6170 - val_loss: 2.0865\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.08665 to 2.08646, saving model to model.h5\n",
      "Epoch 20/20\n",
      " - 71s - loss: 0.5437 - val_loss: 2.0831\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.08646 to 2.08315, saving model to model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea9c064e80>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=32, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATASET:\n",
      "\n",
      "German: ist das fur mich,\n",
      "Correct English Translation=is it for me,\n",
      "Predicted English Translation=is it for me\n",
      "\n",
      "German: ich habe kaviar gegessen,\n",
      "Correct English Translation=i ate caviar,\n",
      "Predicted English Translation=i ate caviar\n",
      "\n",
      "German: das ist eine pflanze,\n",
      "Correct English Translation=its a plant,\n",
      "Predicted English Translation=its a plant\n",
      "\n",
      "German: das ist ein tisch,\n",
      "Correct English Translation=that is a table,\n",
      "Predicted English Translation=thats a a\n",
      "\n",
      "German: sie schoss auf ihn,\n",
      "Correct English Translation=she shot him,\n",
      "Predicted English Translation=she adores him\n",
      "\n",
      "German: du bist durchtrieben,\n",
      "Correct English Translation=youre sharp,\n",
      "Predicted English Translation=youre sharp\n",
      "\n",
      "German: entledigen sie sich ihrer,\n",
      "Correct English Translation=get rid of her,\n",
      "Predicted English Translation=get rid of her\n",
      "\n",
      "German: rate wer ich bin,\n",
      "Correct English Translation=guess who i am,\n",
      "Predicted English Translation=save what i am\n",
      "\n",
      "German: tom kann antworten,\n",
      "Correct English Translation=tom can answer,\n",
      "Predicted English Translation=tom can yes\n",
      "\n",
      "German: ich bin so glucklich,\n",
      "Correct English Translation=im so happy,\n",
      "Predicted English Translation=im am happy\n",
      "\n",
      "BLEU Score: 0.119900\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences after training\n",
    "print('TRAINING DATASET:\\n')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING DATASET:\n",
      "\n",
      "German: tom kennt pistolen,\n",
      "Correct English Translation=tom knows guns,\n",
      "Predicted English Translation=tom knows her\n",
      "\n",
      "German: tom ist brav,\n",
      "Correct English Translation=tom is good,\n",
      "Predicted English Translation=toms is\n",
      "\n",
      "German: tom hat verloren,\n",
      "Correct English Translation=tom lost,\n",
      "Predicted English Translation=tom stopped\n",
      "\n",
      "German: wer ist gefallen,\n",
      "Correct English Translation=who fell,\n",
      "Predicted English Translation=who swam\n",
      "\n",
      "German: tom kennt mich,\n",
      "Correct English Translation=tom knows me,\n",
      "Predicted English Translation=tom knows me\n",
      "\n",
      "German: sie sind wach,\n",
      "Correct English Translation=theyre awake,\n",
      "Predicted English Translation=theyre sharp\n",
      "\n",
      "German: dort ist tom,\n",
      "Correct English Translation=theres tom,\n",
      "Predicted English Translation=here is tom\n",
      "\n",
      "German: wir schauten nach oben,\n",
      "Correct English Translation=we looked up,\n",
      "Predicted English Translation=we tried again\n",
      "\n",
      "German: kannst du mir helfen,\n",
      "Correct English Translation=can you help me,\n",
      "Predicted English Translation=can you help me\n",
      "\n",
      "German: es muss hier sein,\n",
      "Correct English Translation=it must be here,\n",
      "Predicted English Translation=it was be home\n",
      "\n",
      "BLEU Score: 0.149673\n"
     ]
    }
   ],
   "source": [
    "# test on some test sequences after training\n",
    "print('TESTING DATASET:\\n')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
